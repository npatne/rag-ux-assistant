12/23/2024
iSTART Early
Internal Usability Test Report 
(Nov - Dec 2024)




Overview
The following report summarizes the process, findings, and recommendations from an internal usability test conducted on the iSTART Early Web Application. The primary focus of the test was on the Teacher's Interface, with some exploration of the Student's Interface from a teacher's perspective. The study utilized the latest development version as of November 22, 2024, and involved eight internal LEI participants in a non-monitored, self-paced setup. Participants were invited via the LEI common email group, volunteered to test the application, and were provided with a detailed testing sheet outlining test cases and scenarios. Feedback was collected granularly on a test case basis in the testing sheet and was followed by a Google form to gather overarching application feedback, with a submission deadline of December 11, 2024.
Key Findings:
1. Participants noted that the application offers a clean and intuitive user interface with a well-structured layout and color scheme.
2. Issues with notifications, tool-tips, instructions, labels, and titles were observed, including inconsistent verbiage, misaligned tone, and grammatical errors, highlighting a need for improved UX copy.
3. Suggested enhancements included adding more confirmation messages for saving complex settings, providing clearer labels in the Student's Interface, and ensuring consistent identification of class information. Additionally, the monotone lesson voices were noted as potentially disengaging for younger students.
4. Development-related concerns included bugs in progress tracking and synchronization between teacher and student interfaces, issues with notification container opacity, and application responsiveness.
________________
Process 
Preparation
The development team conducted an internal test of the application to identify bugs and potential UX enhancements, finalizing a stable version for broader usability testing. Using this version, a master testing worksheet was created. This worksheet included two sheets:
1. User Information and Testing Narrative: Included user credentials, key definitions, and an overview of the testing process.
2. Test Cases: Grouped by scenarios, each test case outlined steps for execution and expected outcomes. Participants recorded details such as test case status, outcome, complexity, usability, comments, and completion time.
Material Prepared
The testing scenarios were designed to reflect typical workflows new teachers might follow, providing realistic context. Key scenarios included:
* Class and Planet Management: Setting up planet orders, configuring learning modules, and verifying them in the student view.
* Login and Classroom Creation: User authentication and initial classroom setup.
* Student Management: Adding and managing student profiles.
* Assignments: Managing and assigning class tasks.
* Student Progress Reports: Tracking student performance.
In total, the testing covered five scenarios with ten tasks, focusing on key areas like login, classroom creation, and assignments.
Distribution
The completed master testing worksheet, along with a Google form, was distributed via email to the LEI members' common group. The email included detailed instructions on how to participate, create a unique copy of the master sheet, and submit findings.
The Google form was designed to collect demographic data and participants' perceptions of the application, with questions evaluating UI elements (e.g., notifications, color scheme, layout, navigation) and open-ended fields for additional feedback.
Collection
Participants were given 15 days to complete the test and provide their feedback. Of the nine participants who volunteered, eight submitted both the completed Google form and master testing sheets, providing valuable insights for further analysis.
Observations 
Eight participants returned their testing sheets and Google forms, while one participant completed only the Google form. The synthesis of data from the forms and testing sheets is organized in the synthesis sheet. Below are the key observations and findings:
* Recorded sentiments leaned slightly toward the negative side, with 13 negative, 6 positive, and 4 neutral comments synthesized from the 8 testing sheets.
* Most negative feedback focused on navigation, information clarity, and specific usability bugs, while positive comments highlighted the clean layout and visually engaging UI.
Strengths of the Application
* Layout: Seven participants rated the layout as "Most Likable," appreciating its clean and engaging design.
* Positive sentiment comments praised features like intuitive navigation in certain areas and visually appealing themes.
 Forms response chart. Question title: How did you find the design of the application?
. Number of responses: . 

Usability Issues
* Navigation: Mixed feedback was received, with four participants marking it as "Most Likable" while others identified unclear or inconsistent areas. Navigation issues were prominently highlighted in the classroom creation task, where users expected the "Create Classroom" feature in the "Class Setup" tab but instead found it under the "Profile" submenu. This issue involved both unclear UX copy and a misplacement of the function.
* Information Clarity: While three participants rated this aspect highly, others highlighted improvements needed in labeling and instructions. Comments pointed out that:
   * Notification messages were unclear.
   * Distinguishable actions had similar or identical names.
   * Confirmation messages were inconsistent in tone and verbiage.
   * Complex tasks in class setup required more information banners and tooltips.
* Other notable issues included the opacity of notification containers and bugs in class creation and synchronization.
Development Issues
* User progress recorded in the student interface did not sync with the teacher interface, particularly in the "Student Progress" task, where participants tested lessons in the student interface and attempted to track progress in the teacher interface.
* Notifications with transparent backgrounds made text difficult to read.
* Duplicate occurrences of usernames were noted in the student search function.
Suggestions and Enhancements
* Users frequently recommended more detailed tooltips, labels, headings, and confirmation messages for complex actions. Improved notifications and dialog boxes to confirm saving changes were also suggested, particularly for class setup tasks.
* One participant noted that the student interface could be more visually and auditorily engaging, especially for younger users.
Demographic Insights
* Most participants were familiar with tools like Canvas or D2L, which likely influenced their ability to complete tasks and their expectations for features.


Recommendations and Next Steps
While the next steps involve addressing the major development bug fixes, the primary recommendation derived from the usability observations is to undertake a detailed effort to create a consistent, clear, and informative UX copy. This includes improving and standardizing page and section headings, navigation labels and elements, confirmation messages, status indicators, action buttons, toggles, and their respective descriptions and tooltips.
The observations highlighted that confusing or unclear verbiage significantly impacted usability across multiple areas. Notifications were often ambiguous, instructions were unclear or absent, and some labels lacked sufficient context. A comprehensive review and improvement of the UX copy throughout the application are essential to address these issues and enhance the overall user experience.